# search geographic twitter data for Hurricane Dorian
# by Joseph Holler, 2019,2021
# This code requires a twitter developer API token!
# See https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html

# install packages for twitter querying and initialize the library

```{r setup, message = FALSE, include = FALSE}
# list of required packages
# we could probably be listing them all here, making the previous block redundant.
packages = c("rtweet","here","dplyr","rehydratoR","svDialogs")

# load and install required packages
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE, quietly=TRUE)
      library(x, character.only = TRUE)
    }
  }
)

# save the R processing environment 
writeLines(capture.output(sessionInfo()),here("procedure","environment","r_environment.txt"))
```

## Setup Twitter Application

reference for search_tweets function: 
https://rtweet.info/reference/search_tweets.html 
don't add any spaces in between variable name and value for your search
e.g. n=1000 is better than n = 1000
the first parameter in quotes is the search string
n=10000 asks for 10,000 tweets
if you want more than 18,000 tweets, change retryonratelimit to TRUE and 
wait 15 minutes for every batch of 18,000
include_rts=FALSE excludes retweets.
token refers to the twitter token you defined above for access to your twitter
developer account
geocode is equal to a string with three parts: longitude, latitude, and 
distance with the units mi for miles or km for kilometers

set up twitter API information with your own information for
app, consumer_key, and consumer_secret
this should launch a web browser and ask you to log in to twitter
for authentication of access_token and access_secret

```{r twitter setup, message = FALSE, include = FALSE}
# Twitter application values
twitter_vars <- list(app = "enter application name",
                     key = "enter application key",
                     secret = "enter application secret")
                    
# if Twitter token has already been created, autofill dialoge with its values
if (exists("twitter_token")) {
  twitter_vars$app <- twitter_token$app$appname
  twitter_vars$key <- twitter_token$app$key
  twitter_vars$secret <- twitter_token$app$secret
}

twitter_token <- create_token(
  app = dlgInput("Twitter App Name:", twitter_vars$app)$res,          
  consumer_key = dlgInput("Twitter Consumer Key:", twitter_vars$key)$res,  		      
  consumer_secret = dlgInput("Twitter Consumer Secret:", twitter_vars$secret)$res,         
  access_token = NULL,
  access_secret = NULL
)
```

# Search for Hurricane Dorian tweets

get tweets for hurricane Dorian, searched on September 11, 2019
**Warning**: this code will no longer result in the same data! 
It is here for reference or replication work only.

```{r search Dorian, message = FALSE, include = FALSE}
dorian_raw = search_tweets("dorian OR hurricane OR sharpiegate",
                       n=200000, include_rts=FALSE,
                       token=twitter_token, 
                       geocode="32,-78,1000mi",
                       retryonratelimit=TRUE) 

# write status id's for results of the original twitter search
write.table(dorian_raw$status_id,
            here("data","raw","public","dorianids.txt"), 
            append=FALSE, quote=FALSE, row.names = FALSE, col.names = FALSE)
```

# Search for generic tweets

get tweets without any text filter for the same geographic region in November, 
searched on November 19, 2019
the query searches for all verified or unverified tweets, i.e. everything

**Warning**: this code will no longer result in the same data! 
It is here for reference or replication work only.

```{r search November, message = FALSE, include = FALSE}
november_raw = search_tweets("-filter:verified OR filter:verified", 
                         n=200000, include_rts=FALSE, 
                         token=twitter_token,
                         geocode="32,-78,1000mi", 
                         retryonratelimit=TRUE)

# write status id's for results of the original twitter search
write.table(november_raw$status_id,
            here("data","raw","public","novemberids.txt"), 
            append=FALSE, quote=FALSE, row.names = FALSE, col.names = FALSE)
```

# Rehydrate the original search results

Twitter does not permit redistribution of Twitter data, with the exception of 
tweet status ids. 
Therefore for the purposes of transparency and reproducibility, 
researchers may include a list of status id's with their publication. 
The process of using those status id's to query Twitter for the full tweet data 
is called **rehydrating**&mdash;like going back to the analogous fire hose of 
big data.
**Warning**: Twitter users and individual tweets can be deleted over time, 
therefore the results of rehydration will be similar, but not identical to, the
original Twitter data used for this research.

```{r rehydrate, message = FALSE, include = FALSE}

# load tweet status id's for Hurricane Dorian search results
dorianids = 
  data.frame(read.table(here("data","raw","public","dorianids.txt"), 
                        numerals = 'no.loss'))

# load cleaned status id's for November general twitter search
novemberids =
  data.frame(read.table(here("data","raw","public","novemberids.txt"),
                        numerals = 'no.loss'))

# rehydrate dorian tweets
dorian_raw = rehydratoR(twitter_token$app$key, twitter_token$app$secret, 
                twitter_token$credentials$oauth_token, 
                twitter_token$credentials$oauth_secret, dorianids, 
                base_path = NULL, group_start = 1)

# rehydrate november tweets
november_raw = rehydratoR(twitter_token$app$key, twitter_token$app$secret, 
                        twitter_token$credentials$oauth_token, 
                        twitter_token$credentials$oauth_secret, novemberids, 
                        base_path = NULL, group_start = 1)
``` 

# Load the original search results

Students in the GEOG 323 Open Source GIScience course may download the original
search results from the private course data repository: 
https://github.com/GIS4DEV/geog323data/raw/main/dorian/dorian_raw.RDS
https://github.com/GIS4DEV/geog323data/raw/main/dorian/november.RDS

Save the two `.RDS` files to the `data/raw/private` folder and then load the
data with the code block below.

```{r load original, message = FALSE, include = FALSE}
dorian_raw = readRDS(here("data", "raw", "private", "dorian_raw.RDS"))
november_raw = readRDS(here("data", "raw", "private", "november_raw.RDS"))
```


# FILTER DORIAN FOR CREATING PRECISE GEOMETRIES 

reference for lat_lng function: https://rtweet.info/reference/lat_lng.html
adds a lat and long field to the data frame, picked out of the fields
that you indicate in the c() list
sample function: lat_lng(x, coords = c("coords_coords", "bbox_coords"))

list and count unique place types
NA results included based on profile locations, not geotagging / geocoding.
If you have these, it indicates that you exhausted the more precise tweets 
in your search parameters and are including locations based on user profiles

```{r count place types, message = FALSE, include = FALSE}
count(dorian_raw, place_type)
```

## Convert geographic information into lat/long coordinates

```{r lat long, message = FALSE, include = FALSE}

# convert GPS coordinates into lat and lng columns
# do not use geo_coords! Lat/Lng will be inverted
dorian = lat_lng(dorian_raw, coords=c("coords_coords"))
november = lat_lng(november_raw, coords=c("coords_coords"))

# select any tweets with lat and lng columns (from GPS) or 
# designated place types of your choosing
dorian = subset(dorian, 
                place_type == 'city'| place_type == 'neighborhood'| 
                  place_type == 'poi' | !is.na(lat))

november = subset(november,
                  place_type == 'city'| place_type == 'neighborhood'| 
                    place_type == 'poi' | !is.na(lat))

# convert bounding boxes into centroids for lat and lng columns
dorian = lat_lng(dorian,coords=c("bbox_coords"))
november = lat_lng(november,coords=c("bbox_coords"))

# re-check counts of place types
count(dorian, place_type)
```

## SAVE FILTERED TWEETS

Save the tweet id's to the `data\derived\public` folder as plain text.  
Save the full tweet data to `data\derived\private` folder as RDS files.  
Full Tweet data cannot be shared with the public, therefore it is stored in
a folder ignored by Git.

```{r savetweetids, message = FALSE, include = FALSE}
write.table(november$status_id,
            here("data","derived","public","novemberids.txt"), 
            append=FALSE, quote=FALSE, row.names = FALSE, col.names = FALSE)

write.table(dorian$status_id,
            here("data","derived","public","dorianids.txt"), 
            append=FALSE, quote=FALSE, row.names = FALSE, col.names = FALSE)

saveRDS(dorian, here("data","derived","private","dorian.RDS"))
saveRDS(november, here("data","derived","private","november.RDS"))
```
